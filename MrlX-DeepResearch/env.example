# ==================== Environment Variables Template ====================
# This is the ONLY environment variable configuration file you need.
# All required and optional settings are organized here.
#
# Quick Start:
# 1. Copy this file: cp env.example .env
# 2. Fill in all REQUIRED values below (marked with your_xxx_here)
# 3. Copy .env to both main and sub containers
# 4. Follow README_QUICKSTART.md Step 0 to prepare models
# 5. Run: bash quick_start.sh main (or sub)

# ==============================================================================
#                              REQUIRED CONFIGURATION
#          User MUST fill in these values (no defaults provided)
# ==============================================================================

# -------------------- Model Checkpoints --------------------
# IMPORTANT: Prepare model checkpoints before training:
# 1. Download Hugging Face format models
# 2. Convert HF format to Megatron torch_dist format for REF_LOAD
#    (See README_QUICKSTART.md Step 0 for detailed conversion instructions)

# Main Agent Model Paths
MAIN_HF_CHECKPOINT=your_main_hf_checkpoint_path_here              # HF format checkpoint
MAIN_REF_LOAD=your_main_ref_load_path_here                        # Megatron torch_dist format (converted)

# Sub Agent Model Paths
SUB_HF_CHECKPOINT=your_sub_hf_checkpoint_path_here                # HF format checkpoint
SUB_REF_LOAD=your_sub_ref_load_path_here                          # Megatron torch_dist format (converted)

# -------------------- Summary LLM API (Main & Sub Agent) --------------------
SUMMARY_LLM_API_KEY=your_summary_api_key_here                     # REQUIRED
SUMMARY_LLM_API_BASE=https://openrouter.ai/api/v1                 # Optional (can change to any provider)
SUMMARY_LLM_MODEL=deepseek/deepseek-chat-v3-0324                  # Optional (can change to any model)

# -------------------- Judge LLM API (Main Agent) --------------------
JUDGE_LLM_API_KEY=your_judge_api_key_here                         # REQUIRED
JUDGE_LLM_API_BASE=https://openrouter.ai/api/v1                   # Optional (can change to any provider)
JUDGE_LLM_MODEL=deepseek/deepseek-chat-v3-0324                    # Optional (can change to any model)

# -------------------- Reasoner LLM API (Main Agent) --------------------
REASONER_LLM_API_KEY=your_reasoner_api_key_here                   # REQUIRED
REASONER_LLM_API_BASE=https://openrouter.ai/api/v1                # Optional (can change to any provider)
REASONER_LLM_MODEL=deepseek/deepseek-chat-v3-0324                 # Optional (can change to any model)

# -------------------- Tool Server LLM API --------------------
TOOL_SERVER_LLM_API_KEY=your_tool_llm_api_key_here                # REQUIRED
TOOL_SERVER_LLM_BASE_URL=https://openrouter.ai/api/v1             # Optional (can change to any provider)
TOOL_SERVER_LLM_MODEL=deepseek/deepseek-chat-v3-0324              # Optional (can change to any model)

# -------------------- Tool Server External APIs --------------------
GOOGLE_SEARCH_KEY=your_google_search_key_here                     # REQUIRED
JINA_API_KEY=your_jina_key_here                                   # REQUIRED

# -------------------- slime Framework --------------------
# slime framework directory (REQUIRED - must be set by user)
SLIME_DIR=your_slime_dir_here                                     # REQUIRED

# -------------------- Network Configuration --------------------
# Sub Agent IP (get from sub container: hostname -i)
SUB_AGENT_IP=your_sub_agent_ip_here                               # REQUIRED


# ==============================================================================
#                             OPTIONAL CONFIGURATION
#                    (Has defaults or auto-generated values)
# ==============================================================================

# -------------------- Main Agent Paths --------------------
# Auto-generated if not specified
# MAIN_CHECKPOINT_DIR=$SLIME_DIR/../checkpoints/main
# MAIN_PROMPT_DATA=$SLIME_DIR/examples/agent-co-train/mock_data_main.jsonl
# MAIN_RAY_TEMP_DIR=$SLIME_DIR/../ray_temp/main

# -------------------- Sub Agent Paths --------------------
# Auto-generated if not specified
# SUB_CHECKPOINT_DIR=$SLIME_DIR/../checkpoints/sub
# SUB_PROMPT_DATA=$SLIME_DIR/examples/agent-co-train/mock_data_sub.jsonl

# -------------------- Database Server --------------------
# Database Server runs on Sub Agent container by default (port 18888)
# Both Main Agent and Sub Agent access it via SUB_AGENT_IP:18888
DATABASE_SERVER_HOST=0.0.0.0
DATABASE_SERVER_PORT=18888

# Database Server IP (only needed if Database Server is on a separate container)
# Default: Uses SUB_AGENT_IP (when Database Server is on same container as Sub Agent)
# If on separate container, uncomment and set to actual IP:
# DATABASE_SERVER_IP=your_database_server_ip_here

# -------------------- Tool Server --------------------
# Tool Server runs on Main Agent container by default (port 50001)
# Main Agent accesses it via localhost
TOOL_SERVER_HOST=0.0.0.0
TOOL_SERVER_PORT=50001

# Tool Server URL (only needed if Tool Server is on a separate container)
# Default: http://localhost:50001/retrieve (when Tool Server is on same container as Main Agent)
# If on separate container, uncomment and set to actual URL:
# RETRIEVAL_SERVICE_URL=http://<tool_server_ip>:50001/retrieve

# Tool Server Performance Settings
MAX_CONCURRENT_REQUESTS=2000
REQUEST_TIMEOUT=180
VISIT_SEMAPHORE_LIMIT=200
SEARCH_SEMAPHORE_LIMIT=500
WEBCONTENT_MAXLENGTH=150000
SEARCH_MAX_QUERIES=3

# -------------------- Logging & Misc --------------------
# Log directory (default: $SLIME_DIR/logs)
# LOG_DIR=your_log_dir_here

# Experiment identifier suffix
KEY_SUFFIX=slime-co-train-test
